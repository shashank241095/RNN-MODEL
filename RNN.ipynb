{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_graph(name):\n",
    "    logdir = os.path.join('log', name)\n",
    "    file_writer = tf.summary.FileWriter(logdir)\n",
    "    with tf.Session() as sess:\n",
    "        file_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....slength [36 15 31 ... 28 34 35]\n",
      "....slength [21 20 13 10 17 44 67 13 23 35 13 34 11 62 29 13 27 20  9 20 30 15 19 30\n",
      " 19 18 21  5 29 18  6 14 22 41 32 16 10 74  9 57 26 15 18 21 45 12 17 46\n",
      " 20 13 16 25 22 23 33  8 16 53 16 33 25  5  8 31 13 29  7 14 11 30 14 12\n",
      " 33 19 13 39 12 31 27  7 26 20 19 29 14 54 21 21  9 24 33 12 25 47 13 12\n",
      "  9 19 27 30 49 27 19 29 19 14  6  8 17 49 18 28 90 26 39 13 22 36  9 27\n",
      " 27  9 12  4 13 19 11 19 20  7  8 52 27 39 11 29  6 19 28  6 14 15 15 34\n",
      " 25  6 38 12 28 21  6 27 18 23 35 23 37 26  9 10  7 21 12 38 48  7 12 18\n",
      " 26  5 66 41 27 31 23 51  7 45 17 34 27 10 10 16 21 22  6 12 19 30 26 23\n",
      " 20 41  5 25 20 12  2 34  9  9  7 17 15 18 15 24 23 12 16 25  8 19 19 33\n",
      " 22 85 17 27 21  4 39 30 10 40 15 33 18 44  8 31 21 16 10 56 33 12 33 22\n",
      " 28 74 16 18  6 10 28 16 27 61 47 12 20 35 15  6 34 16 15  9 23 18 12 21\n",
      " 17 11 25 14 34  9 14  9 39 10 63 15 10  3 30 31 40 14 29 26  6 16 17 46\n",
      " 35 14 58 37 47 31 19 13 31 29  7 29 14 26 14 20 32 37 13 17 32 22 14  6\n",
      " 38 16 22 29 12 19 11 19 10 23 28 21 15  6 11 53 42 13 37 34 16 47 14 26\n",
      " 20 23 21 28 15 27 44  3 40 27 41  7 23 24 49 11 20 13 17 24 20 43 13 13\n",
      "  7  8  9 48 50 22 42  8 32 14  9 25 42 22 17 20 18 36 17 28 31 15 15 19\n",
      " 35 12 14 22 88 10  5 14 21 25 24 14 13 11 29 17 38  6 21  8 15 33 30 16\n",
      "  7  5  9 21 31 16  8  5 40  9 18 22 14 17 61 35 50 30  6 32 21 14 16 11\n",
      " 22 24 37 64 12 31 13 31 31 14 14 23 15 34  4 24 13 21 26 26 35 19 24 29\n",
      "  3 43 29 12 15 31 26 11 15 20  6 22 47  8  8 14 19 13  7 34 19 33 14  8\n",
      " 18 26 19  9 19 25  3 15  9 19 13 23  7 14 26 47 15 31 25 13 16 23 10 23\n",
      " 21 20 11  5 27  8 41]\n",
      "i want 23709\n",
      "WARNING:tensorflow:From <ipython-input-2-bada25931767>:317: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Finished epoch 1. Evaluating ...\n",
      "59.9075601101\n",
      "0.9389957357932295\n",
      "Finished epoch 2. Evaluating ...\n",
      "54.5797739029\n",
      "0.9496127404055348\n",
      "Finished epoch 3. Evaluating ...\n",
      "60.0467629433\n",
      "0.9490905926377164\n",
      "Finished epoch 4. Evaluating ...\n",
      "55.1362900734\n",
      "0.9497867896614742\n",
      "Finished epoch 5. Evaluating ...\n",
      "54.2175421715\n",
      "0.949873814289444\n",
      "Finished epoch 6. Evaluating ...\n",
      "49.671077013\n",
      "0.9484814202419285\n",
      "Finished epoch 7. Evaluating ...\n",
      "49.4547150135\n",
      "0.9491776172656862\n",
      "Finished epoch 8. Evaluating ...\n",
      "53.9888842106\n",
      "0.950134888173353\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "class DatasetReader(object):\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    @staticmethod\n",
    "    def ReadFile(filename, term_index, tag_index):\n",
    "        \n",
    "        \n",
    "        f = open(filename, \"r\")\n",
    "        parsedFile = []\n",
    "        indi = 0\n",
    "        indj = 0\n",
    "        \n",
    "        if bool(term_index):\n",
    "            m= max(term_index.values())\n",
    "            indi = m+1\n",
    "\n",
    "\n",
    "        for i in f:\n",
    "\n",
    "            l = i.strip().split(\" \")\n",
    "\n",
    "            for j in l:\n",
    "\n",
    "                w,t = j.rsplit(\"/\",1)\n",
    "\n",
    "                if w not in term_index:\n",
    "                    if indi in term_index.values():\n",
    "                        indi=indi+1\n",
    "                    term_index[w] = indi\n",
    "                    indi = indi + 1\n",
    "\n",
    "\n",
    "\n",
    "                if t not in tag_index:\n",
    "                    tag_index[t] = indj\n",
    "                    indj = indj + 1\n",
    "\n",
    "\n",
    "\n",
    "        f.close()\n",
    "        ff = open(filename, \"r\")\n",
    "\n",
    "        for it in ff:\n",
    "\n",
    "            parsedLine = []\n",
    "\n",
    "            ll = it.strip().split(\" \")\n",
    "\n",
    "            for jj in ll:\n",
    "                word,tag = jj.rsplit(\"/\",1)\n",
    "\n",
    "                temp = (term_index[word], tag_index[tag])\n",
    "                parsedLine.append(temp)\n",
    "\n",
    "            parsedFile.append(parsedLine)\n",
    "            \n",
    "            \n",
    "        \n",
    "       \n",
    "        \"\"\"Reads file into dataset, while populating term_index and tag_index.\n",
    "     \n",
    "        Args:\n",
    "            filename: Path of text file containing sentences and tags. Each line is a\n",
    "                sentence and each term is followed by \"/tag\". Note: some terms might\n",
    "                have a \"/\" e.g. my/word/tag -- the term is \"my/word\" and the last \"/\"\n",
    "                separates the tag.\n",
    "            term_index: dictionary to be populated with every unique term (i.e. before\n",
    "                the last \"/\") to point to an integer. All integers must be utilized from\n",
    "                0 to number of unique terms - 1, without any gaps nor repetitions.\n",
    "            tag_index: same as term_index, but for tags.\n",
    "\n",
    "        the _index dictionaries are guaranteed to have no gaps when the method is\n",
    "        called i.e. all integers in [0, len(*_index)-1] will be used as values.\n",
    "        You must preserve the no-gaps property!\n",
    "\n",
    "        Return:\n",
    "            The parsed file as a list of lists: [parsedLine1, parsedLine2, ...]\n",
    "            each parsedLine is a list: [(termId1, tagId1), (termId2, tagId2), ...] \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "        return parsedFile\n",
    "\n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    @staticmethod\n",
    "    def BuildMatrices(dataset):\n",
    "        \n",
    "        \n",
    "        \n",
    "        l = [len(x) for x in dataset]\n",
    "        length = max(l)\n",
    "\n",
    "        terms_matrix = []\n",
    "        tags_matrix = []\n",
    "        for sentence in dataset:\n",
    "\n",
    "            # lsmall=len(sentence)\n",
    "\n",
    "            inlist = numpy.zeros((length),dtype=int)\n",
    "            inlist_tag = numpy.zeros((length),dtype=int)\n",
    "\n",
    "            for s in range(0, len(sentence)):\n",
    "                inlist[s] = sentence[s][0]\n",
    "\n",
    "            for ss in range(0, len(sentence)):\n",
    "                inlist_tag[ss] = sentence[ss][1]\n",
    "\n",
    "            terms_matrix.append(inlist)\n",
    "            tags_matrix.append(inlist_tag)\n",
    "            \n",
    "        terms_matrix=numpy.array(terms_matrix,dtype=numpy.int64)\n",
    "        tags_matrix=numpy.array(tags_matrix,dtype=numpy.int64)\n",
    "        l=numpy.array(l,dtype=numpy.int64)\n",
    "\n",
    "\n",
    "            \n",
    "        print \"....slength\",l\n",
    "        tuplee = (terms_matrix, tags_matrix, l)\n",
    "        # terms_matrix= np.zeros((length))\n",
    "\n",
    "        # tags_matrix=np.zeros((length))\n",
    "\n",
    "        \"\"\"Converts dataset [returned by ReadFile] into numpy arrays for tags, terms, and lengths.\n",
    "\n",
    "        Args:\n",
    "            dataset: Returned by method ReadFile. It is a list (length N) of lists:\n",
    "                [sentence1, sentence2, ...], where every sentence is a list:\n",
    "                [(word1, tag1), (word2, tag2), ...], where every word and tag are integers.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of 3 numpy arrays: (terms_matrix, tags_matrix, lengths_arr)\n",
    "                terms_matrix: shape (N, T) int64 numpy array. Row i contains the word\n",
    "                    indices in dataset[i].\n",
    "                tags_matrix: shape (N, T) int64 numpy array. Row i contains the tag\n",
    "                    indices in dataset[i].\n",
    "                lengths: shape (N) int64 numpy array. Entry i contains the length of\n",
    "                    sentence in dataset[i].\n",
    "\n",
    "            T is the maximum length. For example, calling as:\n",
    "                BuildMatrices([[(1,2), (4,10)], [(13, 20), (3, 6), (7, 8), (3, 20)]])\n",
    "            i.e. with two sentences, first with length 2 and second with length 4,\n",
    "            should return the tuple:\n",
    "            (\n",
    "                [[1, 4, 0, 0],    # Note: 0 padding.\n",
    "                 [13, 3, 7, 3]],\n",
    "\n",
    "                [[2, 10, 0, 0],   # Note: 0 padding.\n",
    "                 [20, 6, 8, 20]], \n",
    "\n",
    "                [2, 4]\n",
    "            )\n",
    "        \"\"\"\n",
    "        return tuplee\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def ReadData(train_filename, test_filename=None):\n",
    "        \"\"\"Returns numpy arrays and indices for train (and optionally test) data.\n",
    "\n",
    "       \n",
    "\n",
    "        Args:\n",
    "            train_filename: .txt path containing training data, one line per sentence.\n",
    "                The data must be tagged (i.e. \"word1/tag1 word2/tag2 ...\").\n",
    "            test_filename: Optional .txt path containing test data.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of 3-elements or 4-elements, the later iff test_filename is given.\n",
    "            The first 2 elements are term_index and tag_index, which are dictionaries,\n",
    "            respectively, from term to integer ID and from tag to integer ID. The int\n",
    "            IDs are used in the numpy matrices.\n",
    "            The 3rd element is a tuple itself, consisting of 3 numpy arrsys:\n",
    "                - train_terms: numpy int matrix.\n",
    "                - train_tags: numpy int matrix.\n",
    "                - train_lengths: numpy int vector.\n",
    "                These 3 are identical to what is returned by BuildMatrices().\n",
    "            The 4th element is a tuple of 3 elements as above, but the data is\n",
    "            extracted from test_filename.\n",
    "        \"\"\"\n",
    "        term_index = {'__oov__': 0}  # Out-of-vocab is term 0.\n",
    "        tag_index = {}\n",
    "        \n",
    "        train_data = DatasetReader.ReadFile(train_filename, term_index, tag_index)\n",
    "        train_terms, train_tags, train_lengths = DatasetReader.BuildMatrices(train_data)\n",
    "        \n",
    "        if test_filename:\n",
    "            test_data = DatasetReader.ReadFile(test_filename, term_index, tag_index)\n",
    "            test_terms, test_tags, test_lengths = DatasetReader.BuildMatrices(test_data)\n",
    "\n",
    "            if test_tags.shape[1] < train_tags.shape[1]:\n",
    "                diff = train_tags.shape[1] - test_tags.shape[1]\n",
    "                zero_pad = numpy.zeros(shape=(test_tags.shape[0], diff), dtype='int64')\n",
    "                test_terms = numpy.concatenate([test_terms, zero_pad], axis=1)\n",
    "                test_tags = numpy.concatenate([test_tags, zero_pad], axis=1)\n",
    "            elif test_tags.shape[1] > train_tags.shape[1]:\n",
    "                diff = test_tags.shape[1] - train_tags.shape[1]\n",
    "                zero_pad = numpy.zeros(shape=(train_tags.shape[0], diff), dtype='int64')\n",
    "                train_terms = numpy.concatenate([train_terms, zero_pad], axis=1)\n",
    "                train_tags = numpy.concatenate([train_tags, zero_pad], axis=1)\n",
    "\n",
    "            return (term_index, tag_index,\n",
    "                            (train_terms, train_tags, train_lengths),\n",
    "                            (test_terms, test_tags, test_lengths))\n",
    "        else:\n",
    "            return term_index, tag_index, (train_terms, train_tags, train_lengths)\n",
    "\n",
    "\n",
    "class SequenceModel(object):\n",
    "\n",
    "    def __init__(self, max_length=310, num_terms=1000, num_tags=40):\n",
    "        \"\"\"Constructor. You can add code but do not remove any code.\n",
    "\n",
    "        The arguments are arbitrary: when you are training on your own, PLEASE set\n",
    "        them to the correct values (e.g. from main()).\n",
    "\n",
    "        Args:\n",
    "            max_lengths: maximum possible sentence length.\n",
    "            num_terms: the vocabulary size (number of terms).\n",
    "            num_tags: the size of the output space (number of tags).\n",
    "\n",
    "        You will be passed these arguments by the grader script.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.num_terms = num_terms\n",
    "        self.num_tags = num_tags\n",
    "        self.x = tf.placeholder(tf.int64, [None, self.max_length], 'X')\n",
    "        self.lengths = tf.placeholder(tf.int32, [None], 'lengths')\n",
    "        \n",
    "        self.is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def lengths_vector_to_binary_matrix(self, length_vector):\n",
    "        \"\"\"Returns a binary mask (as float32 tensor) from (vector) int64 tensor.\n",
    "        \n",
    "        Specifically, the return matrix B will have the following:\n",
    "            B[i, :lengths[i]] = 1 and B[i, lengths[i]:] = 0 for each i.\n",
    "        However, since we are using tensorflow rather than numpy in this function,\n",
    "        you cannot set the range as described.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        return tf.sequence_mask(length_vector,maxlen=self.max_length,dtype=tf.float32,name=None)\n",
    "\n",
    "        #return tf.ones([tf.shape(length_vector), self.max_length], dtype=tf.float32)\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"Saves model to a file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"Loads model from a file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def build_inference(self):\n",
    "        \n",
    "\n",
    "        \n",
    "        embeddings = tf.get_variable('embeddings1',[self.num_terms, 55],initializer=None,trainable=True)\n",
    "        x_embeddings = tf.nn.embedding_lookup(embeddings, self.x) \n",
    "        print \"i want\",self.num_terms\n",
    "        \n",
    "        \n",
    "        write_graph(\"step_1\")\n",
    "  \n",
    "    \n",
    "        \n",
    "        #batch=tf.shape(self.x)[0]\n",
    "        #lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True) # can use tf.nn.rnn_cell.GRUCell or tf.nn.rnn_cell.BasicRNNCell instead \n",
    "        \n",
    "        #print lstm_cell.shape\n",
    "        #cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * 3, state_is_tuple=True)\n",
    "        \n",
    "      \n",
    "        #init_state = cells.zero_state(batch,tf.float32)\n",
    "       \n",
    "        #stackedstates, final_state = tf.nn.dynamic_rnn(cells, x_embeddings, initial_state=init_state, sequence_length=self.lengths)\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        # create a RNN cell composed sequentially of a number of RNNCells\n",
    "\n",
    "        # 'outputs' is a tensor of shape [batch_size, max_time, 256]\n",
    "        # 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "        # tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "        \n",
    "  \n",
    "        state_size=110\n",
    "        \n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"bidirectional\") as scope:\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,cell_bw=cell,inputs=x_embeddings,sequence_length=self.lengths,dtype=tf.float32,scope=scope)\n",
    "            x_embeddings=tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        write_graph(\"step_2\")\n",
    "        stackedstates=x_embeddings\n",
    "     \n",
    "        #rnn_cell = tf.keras.layers.SimpleRNNCell(state_size)\n",
    "        \n",
    "        #states=[]\n",
    "        \n",
    "        \n",
    "        #cur_state = tf.zeros(shape=[1, state_size])\n",
    "        #for i in xrange(self.max_length):\n",
    "\n",
    "            #cur_state = rnn_cell(x_embeddings[:, i, :], [cur_state])[0]  # shape (batch, state_size) \n",
    "            #states .append(cur_state)\n",
    "                    \n",
    "\n",
    "        #stackedstates = tf.stack(states, axis=1)\n",
    "      \n",
    "        \n",
    "        with tf.name_scope('logits'):\n",
    "            logits = tf.reshape(stackedstates, [(tf.shape(self.x)[0]*self.max_length),2*state_size])\n",
    "    \n",
    "        \n",
    "  \n",
    "            logits=tf.layers.dense(inputs=logits,units=self.num_tags)      \n",
    "        \n",
    "   \n",
    "            self.logits = tf.reshape(logits, [tf.shape(self.x)[0],self.max_length, self.num_tags])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        write_graph(\"step_3\")\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "        #self.logits=tf.nn.relu(self.logits)\n",
    "\n",
    "        #weights_regularizer=None,biases_initializer=None,biases_regularizer=None)\n",
    "        #logits=tf.contrib.layers.fully_connected(stackedstates,self.num_tags)       \n",
    "        #predictions = tf.argmax(logits, axis=2, name=\"predictions\")\n",
    "        \"\"\"Build the expression from (self.x, self.lengths) to (self.logits).        \n",
    "           Please do not change or override self.x nor self.lengths in this function.\n",
    "\n",
    "        Hint:\n",
    "            - Use lengths_vector_to_binary_matrix\n",
    "            - You might use tf.reshape, tf.cast, and/or tensor broadcasting.\n",
    "        \"\"\"\n",
    "        # TODO(student): make logits an RNN on x.\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #self.logits = tf.zeros([tf.shape(self.x)[0], self.max_length, self.num_tags])\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def run_inference(self, terms, lengths):\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "        logits = self.sess.run(self.logits, {self.x: terms, self.lengths: lengths,self.is_training:False})\n",
    "\n",
    "        return numpy.argmax(logits,axis=-1)\n",
    "        \"\"\"Evaluates self.logits given self.x and self.lengths.\n",
    "        \n",
    "        Hint: This function is straight forward and you might find this code useful:\n",
    "        # logits = session.run(self.logits, {self.x: terms, self.lengths: lengths})\n",
    "        # return numpy.argmax(logits, axis=2)\n",
    "\n",
    "        Args:\n",
    "            terms: numpy int matrix, like terms_matrix made by BuildMatrices.\n",
    "            lengths: numpy int vector, like lengths made by BuildMatrices.\n",
    "\n",
    "        Returns:\n",
    "            numpy int matrix of the predicted tags, with shape identical to the int\n",
    "            matrix tags i.e. each term must have its associated tag. The caller will\n",
    "            *not* process the output tags beyond the sentence length i.e. you can have\n",
    "            arbitrary values beyond length.\n",
    "        \"\"\"\n",
    "        #return numpy.zeros_like(terms)\n",
    "\n",
    "    # TODO(student): You must implement this.\n",
    "    def build_training(self):\n",
    "                            \n",
    "                    \n",
    "        \n",
    "        self.labels=tf.placeholder(tf.int32,[None,self.max_length],name='labels')\n",
    "                            \n",
    "        with tf.name_scope('loss'):\n",
    "        \n",
    "\n",
    "\n",
    "            self.loss=tf.contrib.seq2seq.sequence_loss(self.logits,self.labels,self.lengths_vector_to_binary_matrix(self.lengths))\n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "            self.learning_rate = tf.placeholder_with_default(numpy.array(0.01, dtype='float32'), shape=[], name='None')      \n",
    "            \n",
    "     \n",
    "            tf.losses.add_loss(self.loss,loss_collection=tf.GraphKeys.LOSSES)\n",
    "     \n",
    "\n",
    "        with tf.name_scope('train_step'):\n",
    "            self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_opt = tf.contrib.training.create_train_op(self.loss,self.opt)\n",
    "                                                 \n",
    "                                             \n",
    "        write_graph(\"step_4\")\n",
    "\n",
    "        \"\"\"Prepares the class for training.\n",
    "        \n",
    "        It is up to you how you implement this function, as long as train_on_batch\n",
    "        works.\n",
    "        \n",
    "        Hint:\n",
    "            - Lookup tf.contrib.seq2seq.sequence_loss \n",
    "            - tf.losses.get_total_loss() should return a valid tensor (without raising\n",
    "                an exception). Equivalently, tf.losses.get_losses() should return a\n",
    "                non-empty list.\n",
    "                \n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        self.train = tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.train_summ = tf.summary.scalar(\"seq_to_seq_Loss\", self.loss)\n",
    "                            \n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.step=1\n",
    "        self.s=0\n",
    "\n",
    "        self.train_writer = tf.summary.FileWriter(os.path.join('log', 'train'), self.sess.graph)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    def train_epoch(self, terms, tags, lengths, batch_size=32, learn_rate=1e-2):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        indices = numpy.random.permutation(terms.shape[0])\n",
    "        \n",
    "        \n",
    "        learn_rate=learn_rate/self.step*2\n",
    "       \n",
    " \n",
    "   \n",
    "            \n",
    "        self.step=self.step+1\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        for si in range(0, terms.shape[0], batch_size):\n",
    "            se = min(si + batch_size, terms.shape[0])\n",
    "            slice_x = terms[indices[si:se]] # + 0 to copy slice\n",
    "            slice_y=tags[indices[si:se]]\n",
    "            l=lengths[indices[si:se]]\n",
    "            \n",
    "            \n",
    "            \n",
    "            _, summ=self.sess.run([self.train_opt,self.train_summ],{\n",
    "                self.x: slice_x,\n",
    "                self.labels: slice_y,\n",
    "                self.lengths:l,\n",
    "                self.is_training:True,\n",
    "                self.learning_rate:learn_rate})\n",
    "                    \n",
    "                            \n",
    "\n",
    "            self.train_writer.add_summary(summ, self.s)\n",
    "            self.s += 1\n",
    "           \n",
    "        \n",
    "          \n",
    "        self.train_writer.close()\n",
    "            \n",
    "            #print self.sess.run([self.loss],feed_dict={self.x: slice_x,self.labels: slice_y,self.lengths:l,})\n",
    "        \n",
    "        \"\"\"Performs updates on the model given training training data.\n",
    "        \n",
    "        This will be called with numpy arrays similar to the ones created in \n",
    "        Args:\n",
    "            terms: int64 numpy array of size (# sentences, max sentence length)\n",
    "            tags: int64 numpy array of size (# sentences, max sentence length)\n",
    "            lengths:\n",
    "            batch_size: int indicating batch size. Grader script will not pass this,\n",
    "                but it is only here so that you can experiment with a \"good batch size\"\n",
    "                from your main block.\n",
    "            learn_rate: float for learning rate. Grader script will not pass this,\n",
    "                but it is only here so that you can experiment with a \"good learn rate\"\n",
    "                from your main block.\n",
    "\n",
    "        Return:\n",
    "            boolean. You should return True iff you want the training to continue. If\n",
    "            you return False (or do not return anyhting) then training will stop after\n",
    "            the first iteration!\n",
    "        \"\"\"\n",
    "        \n",
    "        # <-- Your implementation goes here.\n",
    "        # Finally, make sure you uncomment the `return True` below.\n",
    "        return True\n",
    "        pass\n",
    "\n",
    "    # TODO(student): You can implement this to help you, but we will not call it.\n",
    "    def evaluate(self, terms, tags, lengths):\n",
    "        \n",
    "        predicted_tags = self.run_inference(terms, lengths)\n",
    "        \n",
    "        if predicted_tags is None:\n",
    "            print('Is your run_inference function implented?')\n",
    "            return 0\n",
    "        \n",
    "        test_accuracy = numpy.sum(numpy.cumsum(numpy.equal(tags, predicted_tags), axis=1)[numpy.arange(lengths.shape[0]),lengths-1])/numpy.sum(lengths + 0.0)\n",
    "        print test_accuracy\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"This will never be called by us, but you are encouraged to implement it for\n",
    "    local debugging e.g. to get a good model and good hyper-parameters (learning\n",
    "    rate, batch size, etc).\"\"\"\n",
    "    # Read dataset.\n",
    "    reader = DatasetReader\n",
    "    train_filename = 'hmm-training-data/ja_gsd_train_tagged.txt'\n",
    "    test_filename = 'hmm-training-data/ja_gsd_dev_tagged.txt'\n",
    "    term_index, tag_index, train_data, test_data = reader.ReadData(train_filename, test_filename)\n",
    "    (train_terms, train_tags, train_lengths) = train_data\n",
    "    (test_terms, test_tags, test_lengths) = test_data\n",
    "\n",
    "    model = SequenceModel(train_tags.shape[1], len(term_index), len(tag_index))\n",
    "    model.build_inference()\n",
    "    model.build_training()\n",
    "    for j in xrange(8):\n",
    "        start = time.time()\n",
    "        model.train_epoch(train_terms, train_tags, train_lengths)        \n",
    "        print('Finished epoch %i. Evaluating ...' % (j+1))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        model.evaluate(test_terms, test_tags, test_lengths)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
